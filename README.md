# Tensorflow for Deep Learning:
## Description:
This repo is my hands-on experience with the Zero to Mastery TensorFlow for Deep Learning course.
It covers a lot of concepts and techniques of how to build a good ML application, starting from planning our problem and decide what is the model’s inputs and outputs, passing through getting and preparing our data in a numeric form with specific shapes in a performant way, building and experimenting different models’ architectures, visualizing and modifying models, and finally using the best performing model with real data.  

### Content:
- [**01_Regression_with_Tensorflow**](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/01_Regression_with_Tensorflow.ipynb): it covers an introduction about regression problems ,It introduces defining and splitting data, concept of a model and its architecture (neurons, layers, hyperparameters,....), different visualization/plotting methods to understand the behavior of the model, how to improve that behavior (model’s performance) from several aspects,and  how preprocessing (normalize/standardize) data could improve the performance.
- [**02_NN_Classification:**](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/02_NN_Classification.ipynb)  covers binary/multi-class classification,it starts with an intro to binary classification, plotting the decision boundary, introduces the concept of Non-linearity and how it improves the classification process.
It shows how to pick the best learning rate while training by introducing ‘callbacks’.
Then, it uses tensorflow multi-class datasets to import Fashion MNIST data to work on a multi-classification problem. 
- [**03_section3_CNN_pt1**:](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/section3_CNN.ipynb) it covers an introduction to classification problems with Convolutional Neural Networks, this is the first part which is about  binary problem replicates and end-to-end way to model  pizza_steak dataset with a convolutional neural network ,It covers: preprocessing images in batches/scaling with `tf.keras.preprocessing.image.ImgaeDataGenerator` to have a data of images/labels and also used to do data Augmentation, CNN layers (Conv2D, MaxPooling,....)
- [**03_CNN_MulticlassClassification:**](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/03_CNN_MulticlassClassification.ipynb) the second part of CNN module, Generalizes the previous binary problem to be a Multi-class classification problem with a subset of the Food101 dataset.
- **[04_Transfer_learning_feature_extraction:](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/Transfer_learning_feature_extraction.ipynb)**  introduces the Feature extraction transfer learning using the “ResNetV50”, and “EfficientNet50” pretrained models from “Tensorflow Hub” and “KerasLayer()” with the same previous problem.
- [**05_Transferlearning_Fine_Tuning:**]() introduces “fine tuning” transfer learning and modifies the previous models, Keras Functional API , using augmentation layers,  and ” ModelCheckpoint” and “TensorBoard” callbacks  to save intermediate training results, and compare models’ results.
- [**06_FoodVision_mini:**](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/6_FoodVision_mini.ipynb) gathers the knowledge of previous two parts to train on all classes of the 10% Food101 data, and makes predictions with on custom images of food.
- [**07_Milestone_Project1:**](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/7_Milestone_Project1.ipynb) introduces tensorflow datasets (TFDS) to download the complete version of Food101 dataset, processing the loaded images to be all in the same shape, to be scaled and  in an appropriate type. Using the `tf.data` API to batch, prefetch and load the data in a performant way. Introduce the “Mixed Precision training” concept to use a mix of float16 and float32 tensors to make better use of our GPU's memory (using less memory per tensor means more tensors can be computed on simultaneously) by `tf.keras.mixed_precision`. Building the model using transfer learning in the order of Building a feature extraction model (replace the top few layers of a pretrained model), Training for a few epochs with lower layers frozen, and Fine-tune if necessary with multiple layers unfrozen. Use multiple callbacks while training (tensor_board, modelcheckpoint, `EarlyStopping` to stop training at some point, `ReduceLROnPlateau` to tune the learning rate)
- [**08_Introduction_to_NLP:**](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/8_Introduction_to_NLP.ipynb)  it works on classification of whether a tweet is a disaster (NLP binary classification problem).
To do so, it covers converting text into numbers using “tokenization” by `tf.keras.layers.TextVectorization`, and turning the tokenized text into an “Embedding” by ` tf.keras.layers.Embedding` ,modeling a text dataset starting with a baseline (TF-IDF) and then building several deep learning text models (Dense, RNN models like LSTM, and GRU, Conv1D which using CNN with text (1D-data), and  Transfer learning to use a pre-trained Embeddings), comparing the performance of each model based on the evaluation metrics (Accuracy, Precision, Recall, F1-score) and using tensorboard, combining all models into an ensemble, saving and loading a trained model, and finding the most wrong predictions to detect problems in dataset.
- [**09_Milestone_Project_2_Skimlit:**](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/9_Milestone_Project_2_Skimlit.ipynb)  replicates the deep learning model behind the ‘2017 paper PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts’. Creates an NLP model to classify abstract sentences into the role they play (e.g. objective, methods, results, etc) .
It covers downloading a text dataset (PubMed RCT200k from GitHub), preprocessing data to prepare them for modeling, Setting up a series of modeling experiments,Making a baseline (TF-IDF classifier),Deep models with different combinations of: token embeddings, character embeddings, pretrained embeddings, positional embeddings, Building our first multimodal model (taking multiple types of data inputs),Replicating the model architecture from the paper ,Findinding the most wrong predictions, and Making predictions on PubMed abstracts from the wild.
Introduces how to use multiple inputs for a model, concatenating models to build a hybrid-model.
- [**10_Milestone_3_Time_series_forecasting:**](https://github.com/AYA-NASH/NN_with_TensorFlow/blob/main/10_Milestone_3_Time_series_forecasting.ipynb) Introduces time series problems where there is a relation between some data with time.
It works on predicting bitcoin price the next day given a number of previous days.
It starts with getting historical data of bitcoin prices, then read and split these data in an appropriate way, introducing the concept of “window_size” which is the size of data the model will take each time, and the “Horizon” the size of model’s output (target), formatting data in form of windows(features) and horizons (labels) to turn time series data into a supervised learning problem in both of “univariate” and “Multivariate” data. 
Setting up a series of model experiments [ Dense (fully-connected) networks, Sequence models (LSTM and 1D CNN), Ensembling (combining multiple models together) ,Multivariate models (same experiments with multivariate data)], and Replicating the “N-BEATS” algorithm using TensorFlow layer subclassing.
Creating a modeling checkpoint to save the best performing model during training
Making predictions (forecasts) with a time series model
Discussing and creating prediction intervals for time series model forecasts.
